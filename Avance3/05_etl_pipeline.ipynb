{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f00a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "import snowflake.connector\n",
    "\n",
    "# PostgreSQL\n",
    "conn_pg = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"fleetlogix\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Spriest123\"\n",
    ")\n",
    "cur_pg = conn_pg.cursor()\n",
    "\n",
    "# Snowflake\n",
    "conn_sf = snowflake.connector.connect(\n",
    "    user='ENZOZAMBON',\n",
    "    password='AdiiraelSpriest12345',\n",
    "    account='GTUWIRG-PU45327',\n",
    "    warehouse='FLEETLOGIX_WH',\n",
    "    database='FleetLogix_dhw',\n",
    "    schema='ANALYTICS'\n",
    ")\n",
    "cur_sf = conn_sf.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a0f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_in_batches(cursor, sql, data, batch_size=5000, table_name=\"\"):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        cursor.executemany(sql, batch)\n",
    "        conn_sf.commit()\n",
    "        print(f\"[{datetime.now()}] {table_name} - batch {i//batch_size+1} success - {len(batch)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97d849d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2084301068.py:1: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_deliveries = pd.read_sql(\"SELECT * FROM deliveries\", conn_pg)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2084301068.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_trip = pd.read_sql(\"SELECT * FROM trips\", conn_pg)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2084301068.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_vehicle = pd.read_sql(\"SELECT * FROM vehicles\", conn_pg)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2084301068.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_driver = pd.read_sql(\"SELECT * FROM drivers\", conn_pg)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2084301068.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_route = pd.read_sql(\"SELECT * FROM routes\", conn_pg)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2084301068.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_maint = pd.read_sql(\"SELECT vehicle_id, MAX(maintenance_date) AS last_maintenance_date FROM maintenance GROUP BY vehicle_id\", conn_pg)\n"
     ]
    }
   ],
   "source": [
    "df_deliveries = pd.read_sql(\"SELECT * FROM deliveries\", conn_pg)\n",
    "df_trip = pd.read_sql(\"SELECT * FROM trips\", conn_pg)\n",
    "df_vehicle = pd.read_sql(\"SELECT * FROM vehicles\", conn_pg)\n",
    "df_driver = pd.read_sql(\"SELECT * FROM drivers\", conn_pg)\n",
    "df_route = pd.read_sql(\"SELECT * FROM routes\", conn_pg)\n",
    "df_maint = pd.read_sql(\"SELECT vehicle_id, MAX(maintenance_date) AS last_maintenance_date FROM maintenance GROUP BY vehicle_id\", conn_pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b495851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deliveries['scheduled_datetime'] = pd.to_datetime(df_deliveries['scheduled_datetime'])\n",
    "df_trip['departure_datetime'] = pd.to_datetime(df_trip['departure_datetime'])\n",
    "df_trip['arrival_datetime'] = pd.to_datetime(df_trip['arrival_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "254984e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 21:24:07.227150] dim_vehicle - batch 1 success - 200 registros\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Dimensi√≥n VEHICLES ---------------------------\n",
    "# Convertir acquisition_date a datetime\n",
    "df_vehicle['acquisition_date'] = pd.to_datetime(df_vehicle['acquisition_date'], errors='coerce')\n",
    "\n",
    "# Traer la √∫ltima fecha de mantenimiento por veh√≠culo\n",
    "df_maint['last_maintenance_date'] = pd.to_datetime(df_maint['last_maintenance_date'], errors='coerce')\n",
    "df_maint_last = df_maint.groupby('vehicle_id', as_index=False)['last_maintenance_date'].max()\n",
    "\n",
    "# Merge seguro con df_vehicle\n",
    "if 'last_maintenance_date' in df_vehicle.columns:\n",
    "    df_vehicle = df_vehicle.drop(columns=['last_maintenance_date'])\n",
    "\n",
    "df_vehicle = df_vehicle.merge(df_maint_last, on='vehicle_id', how='left')\n",
    "\n",
    "# Calcular edad en meses\n",
    "df_vehicle['age_months'] = ((pd.Timestamp.today() - df_vehicle['acquisition_date']).dt.days // 30).fillna(0).astype(int)\n",
    "\n",
    "# Asegurarse de que last_maintenance_date exista\n",
    "if 'last_maintenance_date' not in df_vehicle.columns:\n",
    "    df_vehicle['last_maintenance_date'] = pd.NaT\n",
    "\n",
    "# Convertir fechas a string para Snowflake\n",
    "df_vehicle['acquisition_date'] = df_vehicle['acquisition_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_vehicle['last_maintenance_date'] = df_vehicle['last_maintenance_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Preparar registros para insert\n",
    "vehicle_records = list(df_vehicle[['vehicle_id','license_plate','vehicle_type','capacity_kg','fuel_type',\n",
    "                                   'acquisition_date','age_months','status','last_maintenance_date']].itertuples(index=False, name=None))\n",
    "\n",
    "# Insertar en batches\n",
    "insert_in_batches(cur_sf, \"\"\"\n",
    "INSERT INTO dim_vehicle (vehicle_id, license_plate, vehicle_type, capacity_kg, fuel_type, acquisition_date, age_months, status, last_maintenance_date)\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "\"\"\", vehicle_records, batch_size=5000, table_name=\"dim_vehicle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "491c4ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 21:35:35.602156] dim_driver - batch 1 success - 400 registros\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Dimensi√≥n DRIVERS ---------------------------\n",
    "\n",
    "# Convertir fechas a datetime\n",
    "df_driver['hire_date'] = pd.to_datetime(df_driver['hire_date'], errors='coerce')\n",
    "df_driver['license_expiry'] = pd.to_datetime(df_driver['license_expiry'], errors='coerce')\n",
    "\n",
    "# Calcular experiencia en meses\n",
    "df_driver['experience_months'] = ((pd.Timestamp.today() - df_driver['hire_date']).dt.days // 30).fillna(0).astype(int)\n",
    "\n",
    "# Crear full_name\n",
    "df_driver['full_name'] = df_driver['first_name'].fillna('') + ' ' + df_driver['last_name'].fillna('')\n",
    "\n",
    "# Contar viajes por conductor\n",
    "df_trip_counts = df_trip.groupby('driver_id').size().reset_index(name='total_trips')\n",
    "\n",
    "# Hacer merge\n",
    "if 'total_trips' in df_driver.columns:\n",
    "    df_driver = df_driver.drop(columns=['total_trips'])\n",
    "\n",
    "df_driver = df_driver.merge(df_trip_counts, on='driver_id', how='left')\n",
    "\n",
    "# Rellenar NaN con 0\n",
    "df_driver['total_trips'] = df_driver['total_trips'].fillna(0)\n",
    "df_driver['completed_trips'] = df_driver['total_trips']\n",
    "\n",
    "# Categor√≠a de desempe√±o evitando divisi√≥n por 0\n",
    "df_driver['performance_category'] = np.where(\n",
    "    df_driver['total_trips'] == 0, 'Bajo',  # Si no hizo viajes\n",
    "    np.where(df_driver['completed_trips']/df_driver['total_trips'] > 0.7, 'Alto',\n",
    "             np.where(df_driver['completed_trips']/df_driver['total_trips'] > 0.5, 'Medio','Bajo'))\n",
    ")\n",
    "    \n",
    "# Convertir fechas a string para Snowflake (maneja NaT)\n",
    "df_driver['hire_date'] = df_driver['hire_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "df_driver['license_expiry'] = df_driver['license_expiry'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Reemplazar NaN por None para Snowflake\n",
    "df_driver = df_driver.where(pd.notnull(df_driver), None)\n",
    "\n",
    "# Preparar registros\n",
    "driver_records = list(df_driver[['driver_id', 'employee_code','full_name','license_number','license_expiry',\n",
    "                                 'phone','hire_date','experience_months','status','performance_category']].itertuples(index=False, name=None))\n",
    "\n",
    "# Insertar en batches\n",
    "insert_in_batches(cur_sf, \"\"\"\n",
    "INSERT INTO dim_driver (driver_id, employee_code, full_name, license_number, license_expiry, phone, hire_date, experience_months, status, performance_category)\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "\n",
    "\"\"\", driver_records, batch_size=5000, table_name=\"dim_driver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f17143a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 21:35:37.915123] dim_route - batch 1 success - 50 registros\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Dimensi√≥n ROUTES ---------------------------\n",
    "# Llenar valores nulos en num√©ricos\n",
    "df_route[['distance_km','estimated_duration_hours','toll_cost']] = df_route[['distance_km','estimated_duration_hours','toll_cost']].fillna(0)\n",
    "\n",
    "# Asegurar tipos\n",
    "df_route['distance_km'] = df_route['distance_km'].astype(float)\n",
    "df_route['estimated_duration_hours'] = df_route['estimated_duration_hours'].astype(float)\n",
    "df_route['toll_cost'] = df_route['toll_cost'].astype(float)\n",
    "\n",
    "# Renombrar columnas si es necesario\n",
    "df_route = df_route.rename(columns={\n",
    "    'origin': 'origin_city',\n",
    "    'destination': 'destination_city'\n",
    "})\n",
    "\n",
    "# Preparar registros\n",
    "route_records = list(df_route[['route_id', 'route_code','origin_city','destination_city','distance_km','estimated_duration_hours','toll_cost']].itertuples(index=False, name=None))\n",
    "\n",
    "# Insertar en batches\n",
    "insert_in_batches(cur_sf, \"\"\"\n",
    "INSERT INTO dim_route (route_id, route_code, origin_city, destination_city, distance_km, estimated_duration_hours, toll_cost)\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s)\n",
    "\"\"\", route_records, batch_size=5000, table_name=\"dim_route\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ef6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 21:36:06.696098] dim_customer - batch 1 success - 5000 registros\n",
      "[2025-10-14 21:36:09.168827] dim_customer - batch 2 success - 5000 registros\n",
      "[2025-10-14 21:36:11.702932] dim_customer - batch 3 success - 5000 registros\n",
      "[2025-10-14 21:36:14.433855] dim_customer - batch 4 success - 5000 registros\n",
      "[2025-10-14 21:36:16.742602] dim_customer - batch 5 success - 5000 registros\n",
      "[2025-10-14 21:36:19.188247] dim_customer - batch 6 success - 5000 registros\n",
      "[2025-10-14 21:36:21.570374] dim_customer - batch 7 success - 5000 registros\n",
      "[2025-10-14 21:36:23.946140] dim_customer - batch 8 success - 5000 registros\n",
      "[2025-10-14 21:36:26.433988] dim_customer - batch 9 success - 5000 registros\n",
      "[2025-10-14 21:36:28.904192] dim_customer - batch 10 success - 5000 registros\n",
      "[2025-10-14 21:36:30.955389] dim_customer - batch 11 success - 5000 registros\n",
      "[2025-10-14 21:36:33.277792] dim_customer - batch 12 success - 5000 registros\n",
      "[2025-10-14 21:36:35.633953] dim_customer - batch 13 success - 5000 registros\n",
      "[2025-10-14 21:36:38.103379] dim_customer - batch 14 success - 5000 registros\n",
      "[2025-10-14 21:36:40.499420] dim_customer - batch 15 success - 5000 registros\n",
      "[2025-10-14 21:36:43.215141] dim_customer - batch 16 success - 5000 registros\n",
      "[2025-10-14 21:36:45.686431] dim_customer - batch 17 success - 5000 registros\n",
      "[2025-10-14 21:36:48.368338] dim_customer - batch 18 success - 5000 registros\n",
      "[2025-10-14 21:36:50.778127] dim_customer - batch 19 success - 5000 registros\n",
      "[2025-10-14 21:36:52.799887] dim_customer - batch 20 success - 5000 registros\n",
      "[2025-10-14 21:36:55.034402] dim_customer - batch 21 success - 5000 registros\n",
      "[2025-10-14 21:36:57.759951] dim_customer - batch 22 success - 5000 registros\n",
      "[2025-10-14 21:37:00.364850] dim_customer - batch 23 success - 5000 registros\n",
      "[2025-10-14 21:37:02.573000] dim_customer - batch 24 success - 5000 registros\n",
      "[2025-10-14 21:37:04.911132] dim_customer - batch 25 success - 5000 registros\n",
      "[2025-10-14 21:37:05.698498] dim_customer - batch 26 success - 828 registros\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Dimensi√≥n CUSTOMER ---------------------------\n",
    "df_trip = df_trip.merge(df_route[['route_id','destination_city']], on='route_id', how='left')\n",
    "df_customer = df_deliveries.merge(df_trip[['trip_id','destination_city']], on='trip_id', how='left')\n",
    "df_customer_group = df_customer.groupby(['customer_name','destination_city']).size().reset_index(name='total_deliveries')\n",
    "\n",
    "def cat_func(x):\n",
    "    if x==1: return 'Ocasional'\n",
    "    elif x==2: return 'Regular'\n",
    "    else: return 'Habitual'\n",
    "\n",
    "df_customer_group['customer_category'] = df_customer_group['total_deliveries'].apply(cat_func)\n",
    "customers_dim = [(i+1, row['customer_name'], row['destination_city'], datetime.today(), row['total_deliveries'], row['customer_category']) \n",
    "                 for i,row in df_customer_group.iterrows()]\n",
    "\n",
    "insert_in_batches(cur_sf, \"\"\"\n",
    "INSERT INTO dim_customer (customer_key, customer_name, city, first_delivery_date, total_deliveries, customer_category)\n",
    "VALUES (%s,%s,%s,%s,%s,%s)\n",
    "\"\"\", customers_dim, batch_size=5000, table_name=\"dim_customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dd8913b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 21:37:08.449184] dim_date - batch 1 success - 1018 registros\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Dimensi√≥n DATE ---------------------------\n",
    "all_dates = set(df_deliveries['scheduled_datetime'].dt.date)\n",
    "dim_date = []\n",
    "\n",
    "for dt in all_dates:\n",
    "    dim_date.append((\n",
    "        int(dt.strftime(\"%Y%m%d\")),\n",
    "        datetime.combine(dt, datetime.min.time()),\n",
    "        dt.isoweekday(),\n",
    "        dt.strftime(\"%A\"),\n",
    "        dt.day,\n",
    "        dt.timetuple().tm_yday,\n",
    "        dt.isocalendar()[1],\n",
    "        dt.month,\n",
    "        dt.strftime(\"%B\"),\n",
    "        (dt.month-1)//3+1,\n",
    "        dt.year,\n",
    "        dt.weekday()>=5,\n",
    "        0, None,\n",
    "        (dt.month-1)//3+1,\n",
    "        dt.year\n",
    "    ))\n",
    "\n",
    "insert_in_batches(cur_sf, \"\"\"\n",
    "INSERT INTO dim_date (date_key, full_date, day_of_week, day_name, day_of_month, day_of_year, week_of_year, month_num, month_name, quarter, year, is_weekend, is_holiday, holiday_name, fiscal_quarter, fiscal_year)\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "\"\"\", dim_date, batch_size=5000, table_name=\"dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cedee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 21:37:07.132687] dim_time - batch 1 success - 1440 registros\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Dimensi√≥n TIME ---------------------------\n",
    "dim_time = []\n",
    "\n",
    "for h in range(24):\n",
    "    for m in range(60):\n",
    "        time_key = h*10000 + m*100\n",
    "        am_pm = 'AM' if h < 12 else 'PM'\n",
    "        hour_24 = f\"{h:02d}:{m:02d}\"\n",
    "        hour_12 = f\"{h%12 if h%12!=0 else 12:02d}:{m:02d} {am_pm}\"\n",
    "        tod = 'Madrugada' if 0 <= h < 6 else 'Ma√±ana' if 6 <= h < 12 else 'Tarde' if 12 <= h < 18 else 'Noche'\n",
    "        is_business = 8 <= h <= 18\n",
    "        shift = 'Turno 1' if 0 <= h < 8 else 'Turno 2' if 8 <= h < 16 else 'Turno 3'\n",
    "        dim_time.append((time_key, h, m, 0, tod, hour_24, hour_12, am_pm, is_business, shift))\n",
    "\n",
    "insert_in_batches(cur_sf, \"\"\"\n",
    "INSERT INTO dim_time (time_key, hour, minute, second, time_of_day, hour_24, hour_12, am_pm, is_business_hour, shift)\n",
    "VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\n",
    "\"\"\", dim_time, batch_size=5000, table_name=\"dim_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d09020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 4.0.0, Python Version: 3.13.0, Platform: Windows-10-10.0.19043-SP0\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2529720248.py:79: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn_pg)\n",
      "INFO:__main__:üì• Datos extra√≠dos: 125850 filas\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2529720248.py:104: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_vehicle = pd.read_sql(\"SELECT VEHICLE_ID, VEHICLE_KEY FROM DIM_VEHICLE\", conn_sf)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2529720248.py:105: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_driver = pd.read_sql(\"SELECT DRIVER_ID, DRIVER_KEY FROM DIM_DRIVER\", conn_sf)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2529720248.py:106: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_route = pd.read_sql(\"SELECT ROUTE_ID, ROUTE_KEY FROM DIM_ROUTE\", conn_sf)\n",
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_22124\\2529720248.py:107: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_customer = pd.read_sql(\"SELECT CUSTOMER_NAME, CUSTOMER_KEY FROM DIM_CUSTOMER\", conn_sf)\n",
      "INFO:__main__:üîÑ Transformando FACT_DELIVERIES\n",
      "INFO:__main__:üìä FACT_DELIVERIES transformada: 252096 registros\n",
      "INFO:__main__:‚úÖ Insertadas 252096 filas en FACT_DELIVERIES\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# üì¶ FACT DELIVERIES - ETL\n",
    "# ==========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import psycopg2\n",
    "import snowflake.connector\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Logging\n",
    "# ----------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Par√°metros de ETL\n",
    "# ----------------------------------------------------\n",
    "etl_run_id = 1\n",
    "fuel_price = 3.5  # Precio promedio por litro de combustible\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Conexi√≥n a PostgreSQL (origen)\n",
    "# ----------------------------------------------------\n",
    "conn_pg = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"fleetlogix\",\n",
    "    user=\"postgres\",\n",
    "    password=\"Spriest123\"\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Conexi√≥n a Snowflake (destino)\n",
    "# ----------------------------------------------------\n",
    "conn_sf = snowflake.connector.connect(\n",
    "    user='ENZOZAMBON',\n",
    "    password='AdiiraelSpriest12345',\n",
    "    account='GTUWIRG-PU45327',\n",
    "    warehouse='FLEETLOGIX_WH',\n",
    "    database='FleetLogix_dhw',\n",
    "    schema='ANALYTICS'\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Extracci√≥n de datos hist√≥ricos\n",
    "# ----------------------------------------------------\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    d.delivery_id,\n",
    "    d.trip_id,\n",
    "    d.tracking_number,\n",
    "    d.customer_name,\n",
    "    d.delivery_address,\n",
    "    d.package_weight_kg,\n",
    "    d.scheduled_datetime,\n",
    "    d.delivered_datetime,\n",
    "    d.delivery_status,\n",
    "    d.revenue,\n",
    "    t.vehicle_id,\n",
    "    t.driver_id,\n",
    "    t.route_id,\n",
    "    t.departure_datetime AS start_datetime,\n",
    "    t.arrival_datetime AS end_datetime,\n",
    "    t.fuel_consumed_liters AS fuel_consumed_liters,\n",
    "    t.total_weight_kg,\n",
    "    t.status AS trip_status,\n",
    "    r.distance_km,\n",
    "    r.estimated_duration_hours,\n",
    "    r.toll_cost\n",
    "FROM deliveries AS d\n",
    "JOIN trips AS t ON d.trip_id = t.trip_id\n",
    "JOIN routes AS r ON t.route_id = r.route_id\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn_pg)\n",
    "\n",
    "logger.info(f\"üì• Datos extra√≠dos: {len(df)} filas\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Preprocesamiento\n",
    "# ----------------------------------------------------\n",
    "df.columns = df.columns.str.upper()\n",
    "\n",
    "datetime_cols = ['SCHEDULED_DATETIME', 'DELIVERED_DATETIME', 'START_DATETIME', 'END_DATETIME']\n",
    "for col in datetime_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "float_cols = [\n",
    "    'PACKAGE_WEIGHT_KG', 'DISTANCE_KM', 'FUEL_CONSUMED_LITERS',\n",
    "    'ESTIMATED_DURATION_HOURS', 'TOLL_COST', 'REVENUE', 'TOTAL_WEIGHT_KG'\n",
    "]\n",
    "df[float_cols] = df[float_cols].astype(float)\n",
    "\n",
    "df['RECIPIENT_SIGNATURE'] = df['DELIVERY_STATUS'] != 'cancelled'\n",
    "df['REVENUE'] = np.where(df['DELIVERY_STATUS']=='cancelled', 0.0, df['REVENUE'])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Cargar dimensiones desde Snowflake\n",
    "# ----------------------------------------------------\n",
    "df_vehicle = pd.read_sql(\"SELECT VEHICLE_ID, VEHICLE_KEY FROM DIM_VEHICLE\", conn_sf)\n",
    "df_driver = pd.read_sql(\"SELECT DRIVER_ID, DRIVER_KEY FROM DIM_DRIVER\", conn_sf)\n",
    "df_route = pd.read_sql(\"SELECT ROUTE_ID, ROUTE_KEY FROM DIM_ROUTE\", conn_sf)\n",
    "df_customer = pd.read_sql(\"SELECT CUSTOMER_NAME, CUSTOMER_KEY FROM DIM_CUSTOMER\", conn_sf)\n",
    "\n",
    "for d in [df_vehicle, df_driver, df_route, df_customer]:\n",
    "    d.columns = d.columns.str.upper()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Transformaci√≥n\n",
    "# ----------------------------------------------------\n",
    "def transform_fact_deliveries(deliveries_df, dim_vehicles, dim_drivers, dim_routes, dim_customers, etl_run_id):\n",
    "    logger.info(\"üîÑ Transformando FACT_DELIVERIES\")\n",
    "    fact = deliveries_df.copy()\n",
    "\n",
    "    # üî∏ Claves de tiempo y fecha\n",
    "    fact['DATE_KEY'] = pd.to_datetime(fact['DELIVERED_DATETIME']).dt.strftime('%Y%m%d').astype(int)\n",
    "    fact['SCHEDULED_TIME_KEY'] = pd.to_datetime(fact['SCHEDULED_DATETIME']).dt.strftime('%H%M').astype(int)\n",
    "    fact['DELIVERED_TIME_KEY'] = pd.to_datetime(fact['DELIVERED_DATETIME']).dt.strftime('%H%M').astype(int)\n",
    "\n",
    "    # üî∏ Duraciones\n",
    "    fact['DELIVERY_TIME_MINUTES'] = ((fact['DELIVERED_DATETIME'] - fact['SCHEDULED_DATETIME']).dt.total_seconds() / 60).fillna(0).astype(int)\n",
    "    fact['DELAY_MINUTES'] = fact['DELIVERY_TIME_MINUTES'].apply(lambda x: max(0, x - 30))\n",
    "    fact['DELIVERIES_PER_HOUR'] = (60 / fact['DELIVERY_TIME_MINUTES'].clip(lower=1)).round(2)\n",
    "\n",
    "    # üî∏ Variaci√≥n de consumo de combustible (¬±25%)\n",
    "    fact['FUEL_CONSUMED_LITERS'] = fact['FUEL_CONSUMED_LITERS'] * np.random.uniform(0.75, 1.25, len(fact))\n",
    "\n",
    "    # üî∏ Eficiencia\n",
    "    base_efficiency = fact['DISTANCE_KM'] / fact['FUEL_CONSUMED_LITERS'].clip(lower=0.5)\n",
    "    random_factor = np.random.uniform(0.8, 1.2, len(fact))\n",
    "    fact['FUEL_EFFICIENCY_KM_PER_LITER'] = (base_efficiency * random_factor).round(2)\n",
    "\n",
    "# üî∏ M√©tricas financieras ajustadas\n",
    "    base_cost = (fact['FUEL_CONSUMED_LITERS']*fuel_price + fact['TOLL_COST'] + fact['DISTANCE_KM']*0.25 + 10)\n",
    "    fact['COST_PER_DELIVERY'] = (base_cost * np.random.uniform(0.75,1.25,len(fact))).round(2)\n",
    "\n",
    "# Revenue realista: costo + margen 5%-25%\n",
    "    fact['REVENUE_PER_DELIVERY'] = np.where(\n",
    "    fact['DELIVERY_STATUS']=='cancelled', \n",
    "    0.0,\n",
    "    (fact['COST_PER_DELIVERY'] * np.random.uniform(1.05,1.25,len(fact))).round(2)\n",
    ")\n",
    "\n",
    "# Ganancia\n",
    "    fact['PROFIT_PER_DELIVERY'] = (fact['REVENUE_PER_DELIVERY'] - fact['COST_PER_DELIVERY']).round(2)\n",
    "\n",
    "    # üî∏ Indicadores\n",
    "    fact['IS_ON_TIME'] = fact['DELAY_MINUTES'] == 0\n",
    "    fact['IS_DAMAGED'] = np.random.choice([True, False], size=len(fact), p=[0.08, 0.92])\n",
    "    fact['HAS_SIGNATURE'] = fact['RECIPIENT_SIGNATURE']\n",
    "    fact['DELIVERY_STATUS'] = np.where(fact['IS_ON_TIME'], 'ON_TIME', 'DELAYED')\n",
    "\n",
    "    # üî∏ Joins con dimensiones\n",
    "    fact = fact.merge(dim_vehicles[['VEHICLE_ID','VEHICLE_KEY']], on='VEHICLE_ID', how='left')\n",
    "    fact = fact.merge(dim_drivers[['DRIVER_ID','DRIVER_KEY']], on='DRIVER_ID', how='left')\n",
    "    fact = fact.merge(dim_routes[['ROUTE_ID','ROUTE_KEY']], on='ROUTE_ID', how='left')\n",
    "    fact = fact.merge(dim_customers[['CUSTOMER_NAME','CUSTOMER_KEY']], on='CUSTOMER_NAME', how='left')\n",
    "\n",
    "    # üî∏ Control ETL\n",
    "    fact['ETL_BATCH_ID'] = etl_run_id\n",
    "    fact['ETL_TIMESTAMP'] = pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # üî∏ Columnas finales\n",
    "    fact_deliveries = fact[[\n",
    "        'DATE_KEY','SCHEDULED_TIME_KEY','DELIVERED_TIME_KEY',\n",
    "        'VEHICLE_KEY','DRIVER_KEY','ROUTE_KEY','CUSTOMER_KEY',\n",
    "        'DELIVERY_ID','TRIP_ID','TRACKING_NUMBER',\n",
    "        'PACKAGE_WEIGHT_KG','DISTANCE_KM','FUEL_CONSUMED_LITERS',\n",
    "        'DELIVERY_TIME_MINUTES','DELAY_MINUTES','DELIVERIES_PER_HOUR',\n",
    "        'FUEL_EFFICIENCY_KM_PER_LITER','COST_PER_DELIVERY','REVENUE_PER_DELIVERY','PROFIT_PER_DELIVERY',\n",
    "        'IS_ON_TIME','IS_DAMAGED','HAS_SIGNATURE','DELIVERY_STATUS',\n",
    "        'ETL_BATCH_ID','ETL_TIMESTAMP'\n",
    "    ]].copy()\n",
    "\n",
    "    logger.info(f\"üìä FACT_DELIVERIES transformada: {len(fact_deliveries)} registros\")\n",
    "    return fact_deliveries\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üîπ Transformar y cargar\n",
    "# ----------------------------------------------------\n",
    "fact_df = transform_fact_deliveries(df, df_vehicle, df_driver, df_route, df_customer, etl_run_id)\n",
    "\n",
    "success, nchunks, nrows, _ = write_pandas(conn_sf, fact_df, 'FACT_DELIVERIES')\n",
    "logger.info(f\"‚úÖ Insertadas {nrows} filas en FACT_DELIVERIES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40740adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Scheduler activo, ETL se ejecutar√° cada d√≠a a las 10:24\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import time\n",
    "import json\n",
    "\n",
    "# ===============================================\n",
    "# CONFIGURACI√ìN\n",
    "# ===============================================\n",
    "POSTGRES_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'port': 5432,\n",
    "    'database': 'fleetlogix',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Spriest123'\n",
    "}\n",
    "\n",
    "SNOWFLAKE_CONFIG = {\n",
    "    \"user\": 'ENZOZAMBON',\n",
    "    \"password\": 'AdiiraelSpriest12345',\n",
    "    \"account\": 'GTUWIRG-PU45327',\n",
    "    \"warehouse\": 'FLEETLOGIX_WH',\n",
    "    \"database\": 'FleetLogix_dhw',\n",
    "    \"schema\": 'ANALYTICS'\n",
    "}\n",
    "\n",
    "LOAD_HOUR = \"10:24\"\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "# ===============================================\n",
    "# CONEXIONES\n",
    "# ===============================================\n",
    "def connect_postgres():\n",
    "    return psycopg2.connect(**POSTGRES_CONFIG)\n",
    "\n",
    "def connect_snowflake():\n",
    "    return snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
    "\n",
    "# ===============================================\n",
    "# FUNCIONES\n",
    "# ===============================================\n",
    "def insert_staging(conn_sf, df):\n",
    "    \"\"\"Inserta registros en staging_daily_load como VARIANT JSON\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    cur = conn_sf.cursor()\n",
    "    for record in df.to_dict(orient='records'):\n",
    "        raw_json = json.dumps(record, default=str)\n",
    "        cur.execute(\n",
    "            \"INSERT INTO staging_daily_load (raw_data, load_timestamp) VALUES (PARSE_JSON(%s), CURRENT_TIMESTAMP)\",\n",
    "            (raw_json,)\n",
    "        )\n",
    "    cur.close()\n",
    "\n",
    "def get_unique_from_staging(conn_sf, id_field):\n",
    "    \"\"\"\n",
    "    Obtiene registros √∫nicos de staging por id_field usando ROW_NUMBER\n",
    "    Se accede al campo de JSON correctamente con raw_data:id_field\n",
    "    \"\"\"\n",
    "    cur = conn_sf.cursor()\n",
    "    query = f\"\"\"\n",
    "        SELECT raw_data\n",
    "        FROM (\n",
    "            SELECT raw_data,\n",
    "                   ROW_NUMBER() OVER (PARTITION BY raw_data:{id_field} ORDER BY load_timestamp DESC) AS rn\n",
    "            FROM staging_daily_load\n",
    "        )\n",
    "        WHERE rn = 1\n",
    "    \"\"\"\n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    cur.close()\n",
    "\n",
    "    records = []\n",
    "    for r in rows:\n",
    "        v = r[0]\n",
    "        if isinstance(v, dict):\n",
    "            records.append(v)\n",
    "        else:\n",
    "            try:\n",
    "                records.append(json.loads(v))\n",
    "            except:\n",
    "                continue\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def merge_dimension_sf(conn_sf, df, table, key_column):\n",
    "    \"\"\"Upsert simple a Snowflake dimension\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    cur = conn_sf.cursor()\n",
    "    columns = df.columns.tolist()\n",
    "    for i in range(0, len(df), BATCH_SIZE):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO {table} t\n",
    "            USING (SELECT {', '.join(['%s AS '+col for col in columns])}) s\n",
    "            ON t.{key_column} = s.{key_column}\n",
    "            WHEN MATCHED THEN\n",
    "                UPDATE SET {', '.join([f\"{col}=s.{col}\" for col in columns if col != key_column])}\n",
    "            WHEN NOT MATCHED THEN\n",
    "                INSERT ({', '.join(columns)}) VALUES ({', '.join(['s.'+col for col in columns])})\n",
    "        \"\"\"\n",
    "        cur.executemany(merge_sql, batch.values.tolist())\n",
    "    cur.close()\n",
    "    print(f\"‚úÖ Dimensi√≥n {table} actualizada ({len(df)} filas)\")\n",
    "\n",
    "# ===============================================\n",
    "# TRANSFORMACIONES DIMENSIONALES\n",
    "# ===============================================\n",
    "def transform_dim_vehicles(df_vehicle, df_maint):\n",
    "    if df_vehicle is None or df_vehicle.empty:\n",
    "        return pd.DataFrame()\n",
    "    df_vehicle['acquisition_date'] = pd.to_datetime(df_vehicle['acquisition_date'], errors='coerce')\n",
    "    if df_maint is None or df_maint.empty:\n",
    "        df_maint = pd.DataFrame(columns=['vehicle_id','maintenance_date'])\n",
    "    df_maint['maintenance_date'] = pd.to_datetime(df_maint['maintenance_date'], errors='coerce')\n",
    "    last_maint = df_maint.groupby('vehicle_id', as_index=False)['maintenance_date'].max()\n",
    "    df_vehicle = df_vehicle.drop(columns=['last_maintenance_date'], errors='ignore')\n",
    "    if not last_maint.empty:\n",
    "        last_maint = last_maint.rename(columns={'maintenance_date':'last_maintenance_date'})\n",
    "        df_vehicle = df_vehicle.merge(last_maint, on='vehicle_id', how='left')\n",
    "    else:\n",
    "        df_vehicle['last_maintenance_date'] = pd.NaT\n",
    "    df_vehicle['age_months'] = ((pd.Timestamp.today() - df_vehicle['acquisition_date']).dt.days // 30).fillna(0).astype(int)\n",
    "    df_vehicle['acquisition_date'] = df_vehicle['acquisition_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df_vehicle['last_maintenance_date'] = pd.to_datetime(df_vehicle['last_maintenance_date'], errors='coerce') \\\n",
    "                                           .dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df_vehicle = df_vehicle.where(pd.notnull(df_vehicle), None)\n",
    "    return df_vehicle\n",
    "\n",
    "# ===============================================\n",
    "# ETL DIARIO CON STAGING EN SNOWFLAKE\n",
    "# ===============================================\n",
    "def run_daily_etl():\n",
    "    conn_pg = connect_postgres()\n",
    "    conn_sf = connect_snowflake()\n",
    "    \n",
    "    # üîπ Extraer desde PostgreSQL (incremental usando departure_datetime)\n",
    "    fecha_inicio = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df_vehicle = pd.read_sql(\"SELECT * FROM vehicles\", conn_pg)\n",
    "    df_driver = pd.read_sql(\"SELECT * FROM drivers\", conn_pg)\n",
    "    df_route = pd.read_sql(\"SELECT * FROM routes\", conn_pg)\n",
    "    \n",
    "    # Cambio clave: trips usa departure_datetime\n",
    "    df_trip = pd.read_sql(\n",
    "        \"SELECT * FROM trips WHERE departure_datetime > %s\",\n",
    "        conn_pg,\n",
    "        params=[fecha_inicio]\n",
    "    )\n",
    "    \n",
    "    df_deliveries = pd.read_sql(\"SELECT * FROM deliveries\", conn_pg)\n",
    "    df_maint = pd.read_sql(\"SELECT * FROM maintenance\", conn_pg)\n",
    "    \n",
    "    # Insertar en staging\n",
    "    for df in [df_vehicle, df_driver, df_route, df_trip, df_deliveries, df_maint]:\n",
    "        insert_staging(conn_sf, df)\n",
    "    \n",
    "    # Obtener registros √∫nicos\n",
    "    df_vehicle = get_unique_from_staging(conn_sf, 'vehicle_id')\n",
    "    df_driver = get_unique_from_staging(conn_sf, 'driver_id')\n",
    "    df_route = get_unique_from_staging(conn_sf, 'route_id')\n",
    "    df_trip = get_unique_from_staging(conn_sf, 'trip_id')\n",
    "    df_deliveries = get_unique_from_staging(conn_sf, 'delivery_id')\n",
    "    df_maint = get_unique_from_staging(conn_sf, 'vehicle_id')\n",
    "    \n",
    "    # Transformar dimensiones\n",
    "    df_vehicle_t = transform_dim_vehicles(df_vehicle, df_maint)\n",
    "    \n",
    "    # Merge en Snowflake\n",
    "    merge_dimension_sf(conn_sf, df_vehicle_t, 'dim_vehicle', 'vehicle_id')\n",
    "    \n",
    "    # Transformar fact table y cargar\n",
    "    df_fact = df_deliveries.copy()\n",
    "    if not df_fact.empty:\n",
    "        df_fact['ETL_TIMESTAMP'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        write_pandas(conn_sf, df_fact, 'FACT_DELIVERIES')\n",
    "        print(f\"‚úÖ {len(df_fact)} filas insertadas en FACT_DELIVERIES\")\n",
    "    \n",
    "    conn_pg.close()\n",
    "    conn_sf.close()\n",
    "    print(f\"‚úÖ ETL finalizado - {datetime.now()}\")\n",
    "\n",
    "# ===============================================\n",
    "# SCHEDULER\n",
    "# ===============================================\n",
    "schedule.every().day.at(LOAD_HOUR).do(run_daily_etl)\n",
    "print(f\"‚è∞ Scheduler activo, ETL se ejecutar√° cada d√≠a a las {LOAD_HOUR}\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a7fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enzo\\AppData\\Local\\Temp\\ipykernel_12708\\2578783964.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 19.48 litros\n",
      "R¬≤ Score: 0.91\n",
      "Consumo estimado de combustible: 19.36 litros\n"
     ]
    }
   ],
   "source": [
    "#------ EXTRACREDIT ------\n",
    "\n",
    "### El script sirve para predecir el consumo de combustible de un viaje, usando datos hist√≥ricos de entregas, veh√≠culos y rutas. \n",
    "# Esto ayuda a FleetLogix a planificar mejor los viajes, optimizar costos de combustible y mejorar la eficiencia operativa.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import snowflake.connector\n",
    "\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£ Conexi√≥n a Snowflake\n",
    "# ===============================\n",
    "conn = snowflake.connector.connect(\n",
    "    user='ENZOZAMBON',\n",
    "    password='AdiiraelSpriest12345',\n",
    "    account='GTUWIRG-PU45327',\n",
    "    warehouse='FLEETLOGIX_WH',\n",
    "    database='FleetLogix_dhw',\n",
    "    schema='ANALYTICS'\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ Consulta de datos relevantes\n",
    "# ===============================\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    F.DISTANCE_KM,\n",
    "    F.DELIVERY_TIME_MINUTES,\n",
    "    V.VEHICLE_TYPE,\n",
    "    V.CAPACITY_KG,\n",
    "    V.AGE_MONTHS,\n",
    "    R.ESTIMATED_DURATION_HOURS,\n",
    "    F.FUEL_CONSUMED_LITERS\n",
    "FROM FACT_DELIVERIES F\n",
    "JOIN DIM_VEHICLE V ON F.VEHICLE_KEY = V.VEHICLE_KEY\n",
    "JOIN DIM_ROUTE R ON F.ROUTE_KEY = R.ROUTE_KEY\n",
    "WHERE F.FUEL_CONSUMED_LITERS IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Convertir columnas a min√∫sculas para trabajar m√°s c√≥modo en pandas\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# ===============================\n",
    "# 3Ô∏è‚É£ Preparaci√≥n de datos\n",
    "# ===============================\n",
    "X = df.drop(columns=['fuel_consumed_liters'])\n",
    "y = df['fuel_consumed_liters']\n",
    "\n",
    "categorical_features = ['vehicle_type']\n",
    "numerical_features = ['distance_km', 'delivery_time_minutes', 'capacity_kg', 'age_months', 'estimated_duration_hours']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# 4Ô∏è‚É£ Divisi√≥n de datos en train/test\n",
    "# ===============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ Pipeline + Modelo\n",
    "# ===============================\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ===============================\n",
    "# 6Ô∏è‚É£ Evaluaci√≥n\n",
    "# ===============================\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error: {mae:.2f} litros\")\n",
    "print(f\"R¬≤ Score: {r2:.2f}\")\n",
    "\n",
    "# ===============================\n",
    "# 7Ô∏è‚É£ Predicci√≥n de un nuevo viaje\n",
    "# ===============================\n",
    "nuevo_viaje = pd.DataFrame({\n",
    "    'distance_km': [120],\n",
    "    'delivery_time_minutes': [150],\n",
    "    'vehicle_type': ['Cami√≥n mediano'],\n",
    "    'capacity_kg': [3000],\n",
    "    'age_months': [36],\n",
    "    'estimated_duration_hours': [2.5]\n",
    "})\n",
    "\n",
    "consumo_estimado = model.predict(nuevo_viaje)\n",
    "print(f\"Consumo estimado de combustible: {consumo_estimado[0]:.2f} litros\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
